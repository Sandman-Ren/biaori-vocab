# æ•°æ®é‡ç°æ­¥éª¤ - vocabulary.json å®Œæ•´é‡ç°æŒ‡å—

**Project**: ä¸­æ—¥äº¤æµæ ‡å‡†æ—¥æœ¬è¯­ - è¯æ±‡å­¦ä¹ ç³»ç»Ÿ  
**Date**: January 15, 2025  
**Purpose**: æä¾›ä»é›¶å¼€å§‹é‡ç° vocabulary.json çš„è¯¦ç»†æ­¥éª¤å’Œå®Œæ•´å·¥å…·é“¾

## ğŸ¯ ç›®æ ‡

æœ¬æ–‡æ¡£æä¾›äº†å®Œå…¨é‡ç° `vocabulary.json` æ–‡ä»¶çš„è¯¦ç»†æ­¥éª¤ï¼ŒåŒ…æ‹¬ç¯å¢ƒé…ç½®ã€å·¥å…·å®‰è£…ã€æ•°æ®çˆ¬å–ã€å¤„ç†å’ŒéªŒè¯çš„å…¨è¿‡ç¨‹ã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

### ç³»ç»Ÿè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: macOS/Linux/Windows
- **Python**: 3.8+ 
- **Node.js**: 18+
- **å†…å­˜**: æœ€ä½4GBï¼Œæ¨è8GB+
- **ç£ç›˜ç©ºé—´**: è‡³å°‘1GBå¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

### å¼€å‘å·¥å…·
- **ä»£ç ç¼–è¾‘å™¨**: VS Code (æ¨è)
- **Git**: ç‰ˆæœ¬æ§åˆ¶
- **æµè§ˆå™¨**: Chrome/Firefox (ç”¨äºè°ƒè¯•)

## ğŸš€ å®Œæ•´é‡ç°æ­¥éª¤

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

#### 1.1 å…‹éš†é¡¹ç›®
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-username/biaori-vocab.git
cd biaori-vocab

# åˆ›å»ºå·¥ä½œåˆ†æ”¯
git checkout -b data-reproduction
```

#### 1.2 å®‰è£…ä¾èµ–
```bash
# å®‰è£…Node.jsä¾èµ–
npm install

# åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# æˆ–è€… venv\Scripts\activate  # Windows

# å®‰è£…Pythonä¾èµ–
pip install requests beautifulsoup4 lxml pandas json5
```

### ç¬¬äºŒæ­¥ï¼šæ•°æ®çˆ¬å–

#### 2.1 å‡†å¤‡çˆ¬è™«è„šæœ¬

åˆ›å»º `scraper.py`ï¼ˆåŸºäºç°æœ‰åå¤„ç†é€»è¾‘åæ¨ï¼‰:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ—¥äº¤æµæ ‡å‡†æ—¥æœ¬è¯­è¯æ±‡çˆ¬è™«
ä» jp.qsbdc.com çˆ¬å–å®Œæ•´è¯æ±‡æ•°æ®
"""

import requests
import json
import uuid
import time
import re
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BiaoRiVocabScraper:
    def __init__(self):
        self.base_url = "http://jp.qsbdc.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.vocabulary_data = []
        
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text.strip())
    
    def get_lesson_list(self) -> List[Dict]:
        """è·å–æ‰€æœ‰è¯¾ç¨‹åˆ—è¡¨"""
        lessons = []
        
        # æ–°æ ‡å‡†æ—¥æœ¬è¯­è¯¾ç¨‹IDèŒƒå›´ï¼ˆæ ¹æ®ç°æœ‰æ•°æ®æ¨æµ‹ï¼‰
        lesson_ranges = {
            "æ–°æ ‡åˆ": range(1, 100),    # lesson_id 1-99
            "æ–°æ ‡ä¸­": range(100, 200),  # lesson_id 100-199  
            "æ–°æ ‡é«˜": range(200, 350),  # lesson_id 200-349
        }
        
        for book_series, lesson_range in lesson_ranges.items():
            for lesson_id in lesson_range:
                lessons.append({
                    "lesson_id": str(lesson_id),
                    "book_series": book_series,
                    "lesson_url": f"{self.base_url}/jpword/word_list.php?lesson_id={lesson_id}"
                })
                
        logger.info(f"å‡†å¤‡çˆ¬å– {len(lessons)} ä¸ªè¯¾ç¨‹")
        return lessons
    
    def scrape_lesson_vocabulary(self, lesson: Dict) -> List[Dict]:
        """çˆ¬å–å•ä¸ªè¯¾ç¨‹çš„è¯æ±‡"""
        lesson_id = lesson["lesson_id"]
        lesson_url = lesson["lesson_url"]
        
        try:
            response = self.session.get(lesson_url, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è§£æè¯¾ç¨‹åç§°
            lesson_name = self.extract_lesson_name(soup, lesson["book_series"])
            
            # æŸ¥æ‰¾è¯æ±‡è¡¨æ ¼
            vocab_table = soup.find('table', class_='word-table')  # éœ€è¦æ ¹æ®å®é™…HTMLè°ƒæ•´
            if not vocab_table:
                logger.warning(f"è¯¾ç¨‹ {lesson_id} æœªæ‰¾åˆ°è¯æ±‡è¡¨æ ¼")
                return []
            
            words = []
            rows = vocab_table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
            
            for i, row in enumerate(rows, 1):
                try:
                    word_data = self.parse_word_row(row, lesson, lesson_name, str(i))
                    if word_data:
                        words.append(word_data)
                except Exception as e:
                    logger.error(f"è§£æè¯æ±‡è¡Œå¤±è´¥ (è¯¾ç¨‹{lesson_id}, è¡Œ{i}): {e}")
                    continue
            
            logger.info(f"è¯¾ç¨‹ {lesson_id} çˆ¬å–å®Œæˆ: {len(words)} ä¸ªè¯æ±‡")
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            return words
            
        except Exception as e:
            logger.error(f"çˆ¬å–è¯¾ç¨‹ {lesson_id} å¤±è´¥: {e}")
            return []
    
    def extract_lesson_name(self, soup: BeautifulSoup, book_series: str) -> str:
        """æå–è¯¾ç¨‹åç§°"""
        # æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´é€‰æ‹©å™¨
        title_elem = soup.find('h1') or soup.find('title')
        if title_elem:
            title = self.clean_text(title_elem.get_text())
            # æ ¼å¼åŒ–ä¸ºæ ‡å‡†æ ¼å¼ï¼šæ–°æ ‡åˆ_01
            return f"{book_series}_{title}"
        return f"{book_series}_æœªçŸ¥"
    
    def parse_word_row(self, row, lesson: Dict, lesson_name: str, word_number: str) -> Optional[Dict]:
        """è§£æå•ä¸ªè¯æ±‡è¡Œ"""
        cells = row.find_all('td')
        if len(cells) < 5:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
            return None
        
        # æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´ç´¢å¼•
        japanese_word = self.clean_text(cells[0].get_text())
        reading = self.clean_text(cells[1].get_text())
        chinese_meaning = self.clean_text(cells[2].get_text())
        part_of_speech = self.clean_text(cells[3].get_text())
        
        # æå–ä¾‹å¥
        example_sentences = []
        example_cell = cells[4] if len(cells) > 4 else None
        if example_cell:
            examples = example_cell.find_all('div') or [example_cell]
            for example in examples:
                example_text = self.clean_text(example.get_text())
                if example_text:
                    example_sentences.append(example_text)
        
        # æ£€æŸ¥å‘éŸ³URL
        pronunciation_url = None
        audio_link = row.find('a', href=re.compile(r'\.mp3|\.wav'))
        if audio_link:
            pronunciation_url = self.base_url + audio_link['href']
        
        return {
            "_id": str(uuid.uuid4()),
            "book_id": self.get_book_id(lesson_name),
            "lesson_id": lesson["lesson_id"],
            "lesson_name": lesson_name,
            "lesson_url": lesson["lesson_url"],
            "word_number": word_number,
            "japanese_word": japanese_word,
            "reading": reading,
            "pronunciation_url": pronunciation_url,
            "chinese_meaning": chinese_meaning,
            "part_of_speech": part_of_speech,
            "example_sentences": example_sentences,
            "page_url": lesson["lesson_url"],
            "scraped_at": datetime.now().isoformat()
        }
    
    def get_book_id(self, lesson_name: str) -> str:
        """æ ¹æ®è¯¾ç¨‹åç§°æ¨æ–­æ•™æID"""
        if "æ–°æ ‡åˆ" in lesson_name:
            return "7"  # æ–°æ ‡å‡†æ—¥æœ¬è¯­åˆçº§
        elif "æ–°æ ‡ä¸­" in lesson_name:
            return "8"  # æ–°æ ‡å‡†æ—¥æœ¬è¯­ä¸­çº§
        elif "æ–°æ ‡é«˜" in lesson_name:
            return "9"  # æ–°æ ‡å‡†æ—¥æœ¬è¯­é«˜çº§
        return "0"  # æœªçŸ¥æ•™æ
    
    def scrape_all_vocabulary(self) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰è¯æ±‡"""
        lessons = self.get_lesson_list()
        all_vocabulary = []
        
        for i, lesson in enumerate(lessons, 1):
            logger.info(f"çˆ¬å–è¿›åº¦: {i}/{len(lessons)} - è¯¾ç¨‹ {lesson['lesson_id']}")
            
            words = self.scrape_lesson_vocabulary(lesson)
            all_vocabulary.extend(words)
            
            if i % 10 == 0:  # æ¯10ä¸ªè¯¾ç¨‹ä¿å­˜ä¸€æ¬¡
                self.save_data(all_vocabulary, f"backup_lesson_{i}.json")
        
        return all_vocabulary
    
    def save_data(self, data: List[Dict], filename: str = "raw_vocabulary.json"):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}: {len(data)} ä¸ªè¯æ±‡")

def main():
    """ä¸»å‡½æ•°"""
    scraper = BiaoRiVocabScraper()
    
    try:
        # çˆ¬å–æ‰€æœ‰è¯æ±‡
        vocabulary_data = scraper.scrape_all_vocabulary()
        
        # ä¿å­˜åŸå§‹æ•°æ®
        scraper.save_data(vocabulary_data, "raw_vocabulary.json")
        
        logger.info(f"çˆ¬å–å®Œæˆ! æ€»è®¡ {len(vocabulary_data)} ä¸ªè¯æ±‡")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        logger.error(f"çˆ¬å–å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
```

#### 2.2 è¿è¡Œçˆ¬è™«
```bash
# æ¿€æ´»Pythonç¯å¢ƒ
source venv/bin/activate

# è¿è¡Œçˆ¬è™«
python scraper.py

# ç­‰å¾…çˆ¬å–å®Œæˆï¼ˆé¢„è®¡1-2å°æ—¶ï¼‰
# ä¼šç”Ÿæˆ raw_vocabulary.json æ–‡ä»¶
```

### ç¬¬ä¸‰æ­¥ï¼šæ•°æ®åå¤„ç†

#### 3.1 åŠ¨è¯å˜ä½å¤„ç†
```bash
# ä½¿ç”¨ç°æœ‰çš„å˜ä½è„šæœ¬å¤„ç†æ•°æ®
node hydrate-conjugations.js

# è¿™ä¼šå°† raw_vocabulary.json å¤„ç†ä¸º vocabulary.json
# å¹¶ä¸ºæ‰€æœ‰åŠ¨è¯æ·»åŠ å®Œæ•´çš„å˜ä½å½¢å¼
```

#### 3.2 éªŒè¯å¤„ç†ç»“æœ
```bash
# è¿è¡ŒéªŒè¯è„šæœ¬
node verify-conjugations.js

# æ£€æŸ¥è¯­ä¹‰é¡ºåº
node test-semantic-order.js
```

### ç¬¬å››æ­¥ï¼šæ•°æ®éªŒè¯ä¸ä¼˜åŒ–

#### 4.1 éªŒè¯æ•°æ®å®Œæ•´æ€§
```bash
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
ls -la public/data/vocabulary.json

# æ£€æŸ¥è¯æ±‡æ•°é‡ï¼ˆåº”è¯¥çº¦2800+ä¸ªï¼‰
cat public/data/vocabulary.json | jq '. | length'

# æ£€æŸ¥åŠ¨è¯æ•°é‡å’Œå˜ä½
cat public/data/vocabulary.json | jq '[.[] | select(.part_of_speech | test("åŠ¨[123]"))] | length'
```

#### 4.2 è´¨é‡æ£€æŸ¥è„šæœ¬
```bash
# åˆ›å»ºå¿«é€Ÿæ£€æŸ¥è„šæœ¬
cat > check_data_quality.js << 'EOF'
const fs = require('fs');
const data = JSON.parse(fs.readFileSync('public/data/vocabulary.json', 'utf8'));

console.log('=== æ•°æ®è´¨é‡æŠ¥å‘Š ===');
console.log(`æ€»è¯æ±‡æ•°: ${data.length}`);

const verbs = data.filter(item => ['åŠ¨1', 'åŠ¨2', 'åŠ¨3'].includes(item.part_of_speech));
console.log(`åŠ¨è¯æ•°é‡: ${verbs.length}`);

const verbsWithConjugations = verbs.filter(verb => verb.conjugations);
console.log(`å¸¦å˜ä½åŠ¨è¯: ${verbsWithConjugations.length}`);

const books = [...new Set(data.map(item => item.book_id))].sort();
console.log(`æ•™ææ•°é‡: ${books.length} (${books.join(', ')})`);

const lessons = [...new Set(data.map(item => item.lesson_id))].sort((a,b) => parseInt(a) - parseInt(b));
console.log(`è¯¾ç¨‹æ•°é‡: ${lessons.length} (${lessons[0]} - ${lessons[lessons.length-1]})`);

console.log('\n=== æ ·æœ¬æ•°æ® ===');
console.log('ç¬¬ä¸€ä¸ªè¯æ±‡:', JSON.stringify(data[0], null, 2).slice(0, 500) + '...');
EOF

node check_data_quality.js
```

### ç¬¬äº”æ­¥ï¼šæœ€ç»ˆéªŒè¯å’Œéƒ¨ç½²

#### 5.1 å®Œæ•´æ€§æµ‹è¯•
```bash
# è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
npm test  # å¦‚æœæœ‰æµ‹è¯•

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨æµ‹è¯•
npm run dev

# åœ¨æµè§ˆå™¨ä¸­è®¿é—® http://localhost:3000 éªŒè¯åŠŸèƒ½
```

#### 5.2 æ€§èƒ½æµ‹è¯•
```bash
# æ£€æŸ¥æ–‡ä»¶å¤§å°
ls -lh public/data/vocabulary.json

# æµ‹è¯•åŠ è½½æ€§èƒ½
time curl -s http://localhost:3000/data/vocabulary.json > /dev/null
```

#### 5.3 æ„å»ºç”Ÿäº§ç‰ˆæœ¬
```bash
# æ„å»ºé™æ€ç‰ˆæœ¬
npm run build

# éªŒè¯æ„å»ºç»“æœ
ls -la out/
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### é—®é¢˜1ï¼šçˆ¬è™«è¢«åçˆ¬
**ç°è±¡**: HTTP 403/429é”™è¯¯
**è§£å†³**: 
```bash
# å¢åŠ è¯·æ±‚é—´éš”
# åœ¨ scraper.py ä¸­è°ƒæ•´ time.sleep(2)

# æ›´æ¢User-Agent
# åœ¨ session.headers ä¸­æ›´æ–°
```

#### é—®é¢˜2ï¼šç¼–ç é—®é¢˜
**ç°è±¡**: ä¸­æ–‡æˆ–æ—¥æ–‡æ˜¾ç¤ºä¹±ç 
**è§£å†³**:
```bash
# æ£€æŸ¥æ–‡ä»¶ç¼–ç 
file -I public/data/vocabulary.json

# å¼ºåˆ¶UTF-8ç¼–ç 
iconv -f GBK -t UTF-8 vocabulary.json > vocabulary_utf8.json
```

#### é—®é¢˜3ï¼šåŠ¨è¯å˜ä½å¤±è´¥
**ç°è±¡**: verify-conjugations.js æŠ¥å‘Šå˜ä½ç¼ºå¤±
**è§£å†³**:
```bash
# æ¸…é™¤ç¼“å­˜é‡æ–°å¤„ç†
rm public/data/vocabulary.json
node hydrate-conjugations.js

# æ£€æŸ¥ç‰¹å®šåŠ¨è¯
node -e "
const data = require('./public/data/vocabulary.json');
const verb = data.find(item => item.japanese_word === 'è¡Œãã¾ã™');
console.log(JSON.stringify(verb.conjugations, null, 2));
"
```

#### é—®é¢˜4ï¼šæ•°æ®é‡ä¸æ­£ç¡®
**ç°è±¡**: è¯æ±‡æ•°é‡å°‘äºé¢„æœŸ
**è§£å†³**:
```bash
# æ£€æŸ¥çˆ¬è™«æ—¥å¿—
grep "çˆ¬å–å®Œæˆ" scraper.log

# æ£€æŸ¥è¯¾ç¨‹è¦†ç›–
node -e "
const data = require('./public/data/vocabulary.json');
const lessons = [...new Set(data.map(item => item.lesson_id))];
console.log('ç¼ºå¤±è¯¾ç¨‹:', lessons.filter(id => parseInt(id) < 100).length);
"
```

## ğŸ“Š éªŒè¯æ¸…å•

### æ•°æ®å®Œæ•´æ€§
- [ ] æ€»è¯æ±‡æ•° â‰¥ 2800
- [ ] åŠ¨è¯æ•°é‡ â‰¥ 800
- [ ] å˜ä½è¦†ç›–ç‡ â‰¥ 95%
- [ ] æ•™æè¦†ç›–ï¼šæ–°æ ‡åˆã€ä¸­ã€é«˜çº§
- [ ] è¯¾ç¨‹æ•°é‡ â‰¥ 100

### æ•°æ®è´¨é‡
- [ ] æ‰€æœ‰å¿…å¡«å­—æ®µå®Œæ•´
- [ ] æ—¥æ–‡å­—ç¬¦æ˜¾ç¤ºæ­£ç¡®
- [ ] ä¸­æ–‡ç¿»è¯‘å®Œæ•´
- [ ] ä¾‹å¥æ ¼å¼æ­£ç¡®
- [ ] UUIDæ ¼å¼æ­£ç¡®
- [ ] æ—¶é—´æˆ³æ ¼å¼æ­£ç¡®

### åŠŸèƒ½éªŒè¯
- [ ] ç½‘ç«™æ­£å¸¸åŠ è½½
- [ ] ç­›é€‰åŠŸèƒ½æ­£å¸¸
- [ ] æœç´¢åŠŸèƒ½æ­£å¸¸
- [ ] æ¨¡æ€æ¡†æ­£å¸¸æ˜¾ç¤º
- [ ] å¯¼å‡ºåŠŸèƒ½æ­£å¸¸
- [ ] ç§»åŠ¨ç«¯æ˜¾ç¤ºæ­£å¸¸

## ğŸ•’ æ—¶é—´é¢„ä¼°

| æ­¥éª¤ | é¢„ä¼°æ—¶é—´ | è¯´æ˜ |
|------|----------|------|
| ç¯å¢ƒé…ç½® | 30åˆ†é’Ÿ | é¦–æ¬¡é…ç½®ï¼Œåç»­å¯å¤ç”¨ |
| çˆ¬è™«å¼€å‘ | 2-4å°æ—¶ | éœ€è¦åˆ†æç½‘ç«™ç»“æ„ |
| æ•°æ®çˆ¬å– | 1-2å°æ—¶ | ç½‘ç»œé€Ÿåº¦ä¾èµ– |
| åå¤„ç† | 10åˆ†é’Ÿ | è‡ªåŠ¨åŒ–è„šæœ¬ |
| éªŒè¯æµ‹è¯• | 30åˆ†é’Ÿ | æ‰‹åŠ¨æ£€æŸ¥ |
| **æ€»è®¡** | **4-7å°æ—¶** | é¦–æ¬¡å®Œæ•´é‡ç° |

## ğŸ“ é‡ç°è®°å½•æ¨¡æ¿

```markdown
## é‡ç°è®°å½• - [æ—¥æœŸ]

### ç¯å¢ƒä¿¡æ¯
- æ“ä½œç³»ç»Ÿ: 
- Pythonç‰ˆæœ¬: 
- Node.jsç‰ˆæœ¬: 
- çˆ¬å–æ—¶é—´: 

### æ•°æ®ç»Ÿè®¡
- çˆ¬å–è¯¾ç¨‹æ•°: 
- æ€»è¯æ±‡æ•°: 
- åŠ¨è¯æ•°é‡: 
- æ–‡ä»¶å¤§å°: 

### é—®é¢˜è®°å½•
1. 
2. 
3. 

### è§£å†³æ–¹æ¡ˆ
1. 
2. 
3. 

### éªŒè¯ç»“æœ
- [ ] åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡
- [ ] æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡
```

## ğŸ”„ è‡ªåŠ¨åŒ–è„šæœ¬

åˆ›å»ºä¸€é”®é‡ç°è„šæœ¬ï¼š

```bash
#!/bin/bash
# reproduce_vocabulary.sh - ä¸€é”®é‡ç° vocabulary.json

set -e  # é‡åˆ°é”™è¯¯æ—¶åœæ­¢

echo "ğŸš€ å¼€å§‹é‡ç° vocabulary.json..."

# 1. ç¯å¢ƒæ£€æŸ¥
echo "ğŸ“‹ æ£€æŸ¥ç¯å¢ƒ..."
python3 --version
node --version

# 2. å®‰è£…ä¾èµ–
echo "ğŸ“¦ å®‰è£…ä¾èµ–..."
pip install -r requirements.txt
npm install

# 3. æ•°æ®çˆ¬å–
echo "ğŸ•·ï¸ å¼€å§‹æ•°æ®çˆ¬å–..."
python scraper.py

# 4. åå¤„ç†
echo "ğŸ”§ æ•°æ®åå¤„ç†..."
node hydrate-conjugations.js

# 5. éªŒè¯
echo "âœ… éªŒè¯æ•°æ®..."
node verify-conjugations.js
node test-semantic-order.js

# 6. è´¨é‡æ£€æŸ¥
echo "ğŸ“Š è´¨é‡æ£€æŸ¥..."
node check_data_quality.js

echo "ğŸ‰ é‡ç°å®Œæˆ! vocabulary.json å·²ç”Ÿæˆ"
echo "ğŸ“ æ–‡ä»¶ä½ç½®: public/data/vocabulary.json"

# 7. å¯åŠ¨æµ‹è¯•æœåŠ¡å™¨
echo "ğŸŒ å¯åŠ¨æµ‹è¯•æœåŠ¡å™¨..."
npm run dev
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æ•°æ®ç”Ÿå‘½å‘¨æœŸ](data-lifecycle.md) - å®Œæ•´æ•°æ®æµç¨‹
- [æ•°æ®ç»“æ„è§„èŒƒ](data-structure.md) - æ•°æ®æ ¼å¼è¯¦è§£  
- [åå¤„ç†æŒ‡å—](post-processing-guide.md) - Pythonå®ç°å‚è€ƒ
- [ç³»ç»Ÿè®¾è®¡æ–‡æ¡£](../design/system-design.md) - æ•´ä½“æ¶æ„

---

*æœ¬æ–‡æ¡£ç¡®ä¿ vocabulary.json å¯ä»¥å®Œå…¨é‡ç°ã€‚å¦‚é‡é—®é¢˜ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤éƒ¨åˆ†æˆ–æäº¤GitHub Issueã€‚*
